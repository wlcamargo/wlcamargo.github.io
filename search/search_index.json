{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Portfolio Wallace Gra\u00e7a","text":"<p>Welcome to Material for MkDocs.</p>"},{"location":"about/","title":"About Me","text":"<p>In 2019, I began working with Power BI and SQL Server, transitioning Excel reports to Business Intelligence (BI) solutions. In 2020 I became a BI Analyst with a focus on building strategic, but also tactical and operational dashboards. In 2022 I started in the world of big data and all the technologies in this framework have been my main focus of learning and evolution at the moment, as well as the concept of modern data stack.</p>"},{"location":"about/#certifications","title":"Certifications","text":""},{"location":"about/#resume","title":"Resume","text":"<p>  Click here to view my resume  </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#channel-metrics","title":"\ud83d\udcca Channel Metrics","text":""},{"location":"blog/#latest-videos","title":"\ud83d\udcfa Latest videos","text":"Subindo containers com o Docker Compose - Protegendo palavras sens\u00edveisJun 21, 2024 Dashboard Open Source com Apache SupersetJun 3, 2024 Consulta Federada com o TrinoMay 20, 2024 Criando 3 aplica\u00e7\u00f5es Spark e rodando no Docker / Fabric / DatabricksMay 6, 2024 Data Lake VS Lakehouse; O que muda? Como s\u00e3o as tabelas?Apr 21, 2024"},{"location":"contact/","title":"Contact","text":"<p>E-mail: wallacecpdg@gmail.com</p> <p>Phone: + 351 926 802 230</p>"},{"location":"contact/#social-media","title":"Social Media","text":"<p> Linkedin </p> <p> Youtube </p> <p> Github </p> <p> Instagram </p>"},{"location":"location/","title":"Location","text":"<p>Portugal - Aveiro </p> <p></p>"},{"location":"projects/","title":"Projects","text":""},{"location":"projects/#educations-projects","title":"Educations Projects","text":""},{"location":"projects/#sparkanos","title":"Sparkanos","text":"Click here to learn more"},{"location":"projects/#from-zero-to-know-what","title":"From Zero to Know What","text":"Click here to learn more"},{"location":"projects/#publics-projects","title":"Publics Projects","text":"<p>Here you can find some of my main public projects.</p> <p>Warning</p> <p>All projects were built with public or fake data.</p>"},{"location":"projects/#see-my-technical-videos-youtube","title":"See my technical videos.  Youtube","text":""},{"location":"projects/#deploy-google-kubernetes-engine","title":"Deploy Google Kubernetes Engine","text":""},{"location":"projects/#technologies-used-gcp-gke","title":"Technologies Used: <code>GCP, GKE</code>","text":"<ul> <li>Cluster deployment via gcloud</li> <li>Creation of the ingress controller</li> <li>Creation of the application</li> </ul> <p> Repo</p>"},{"location":"projects/#deploy-gcp-with-terraform","title":"Deploy GCP With Terraform","text":""},{"location":"projects/#technologies-used-terraform-gcp-compute-engine","title":"Technologies Used: <code>Terraform, GCP, Compute Engine</code>","text":"<ul> <li>Creating the declarative script</li> <li>Machine deployment</li> </ul> <p> Video |        Repo</p>"},{"location":"projects/#deploy-cloud-run-google-cloud-platform","title":"Deploy Cloud Run (Google Cloud Platform)","text":""},{"location":"projects/#technologies-used-gcp-github-actions-cloud-run","title":"Technologies Used: <code>GCP, Github Actions, Cloud Run</code>","text":"<ul> <li>Automated deployment with GitHub Actions</li> <li>Image versioning in the Registry</li> <li>Application deployment</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#composer-airflow-on-google-cloud-plataform","title":"Composer (Airflow on Google Cloud Plataform)","text":""},{"location":"projects/#technologies-used-gcp-composer-gcs","title":"Technologies Used: <code>GCP, Composer, GCS</code>","text":"<ul> <li>Tutorial on resource creation</li> <li>Building a DAG</li> <li>Running on Composer</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#run-container-with-docker-compose","title":"Run Container With Docker Compose","text":""},{"location":"projects/#technologies-used-docker-compose-postgres-minio","title":"Technologies Used: <code>Docker, Compose, Postgres, Minio</code>","text":"<ul> <li>Creating a declarative Docker Compose file</li> <li>Hiding sensitive words</li> <li>Running 2 containers (Postgres and MinIO)</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#spark-docker-vs-fabric-vs-databricks","title":"Spark Docker vs Fabric vs Databricks","text":""},{"location":"projects/#technologies-used-docker-jupyter-databricks-fabric","title":"Technologies Used: <code>Docker, Jupyter, Databricks, Fabric</code>","text":"<ul> <li>Creating a Spark application in Docker that generates fake data every 10 seconds</li> <li>Creating a Spark application in Docker that reads files from MinIO</li> <li>Creating a Spark application in Docker that generates a bar chart</li> <li>Running the same applications on Databricks</li> <li>Running the same applications on Fabric</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#databricks-connect-api","title":"Databricks Connect API","text":""},{"location":"projects/#technologies-used-databricks-api-delta-table","title":"Technologies Used: <code>Databricks, API, Delta Table</code>","text":"<ul> <li>Import necessary libraries</li> <li>Connect to the API</li> <li>Display results</li> <li>Create database (via Spark SQL script)</li> <li>Materialize data in delta format</li> </ul> <p> Video </p>"},{"location":"projects/#spark-structured-streaming","title":"Spark Structured Streaming","text":""},{"location":"projects/#technologies-used-spark-postgres-json-file","title":"Technologies Used: <code>Spark, Postgres, Json File</code>","text":"<ul> <li>Scan a folder near real time (every 5 seconds)</li> <li>Add checkpoint</li> <li>Process data and write to Postgres</li> </ul> <p> Video</p>"},{"location":"projects/#data-lake-vs-lakehouse","title":"Data Lake vs Lakehouse","text":""},{"location":"projects/#technologies-used-minio-trino","title":"Technologies Used: <code>Minio, Trino</code>","text":"<ul> <li>Difference between Data Lake vs Lakehouse</li> <li>File virtualization with Trino</li> <li>SQL querying on CSV files</li> </ul> <p> Video </p>"},{"location":"projects/#delta-table-time-travel","title":"Delta Table - Time Travel","text":""},{"location":"projects/#technologies-used-delta-table","title":"Technologies Used: <code>Delta Table</code>","text":"<ul> <li>Use of Jupyter Notebook</li> <li>PySpark generating and creating CSV in the landing layer</li> <li>PySpark writing a Delta table in the bronze layer</li> <li>Data alteration in the Delta table</li> <li>Navigating between versions of the Delta table</li> </ul> <p> Video </p>"},{"location":"projects/#delta-table-schema-evolution","title":"Delta Table - Schema Evolution","text":""},{"location":"projects/#technologies-used-delta-table_1","title":"Technologies Used: <code>Delta Table</code>","text":"<ul> <li>Use of Jupyter Notebook</li> <li>PySpark generating and creating CSV in the landing layer</li> <li>PySpark writing a Delta table in the bronze layer</li> <li>Schema change in the Delta table</li> <li>Practical demonstration of schema evolution</li> </ul> <p> Video </p>"},{"location":"projects/#process-near-real-time-google-cloud","title":"Process Near Real Time Google Cloud","text":""},{"location":"projects/#technologies-used-pubsub-cloud-function-bigquery","title":"Technologies Used: <code>Pub/Sub, Cloud Function, BigQuery</code>","text":"<ul> <li>Simulation of sending messages every 1 minute</li> <li>Topic creation in Pub/Sub</li> <li>Function creation in Cloud Functions</li> <li>Writing data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#analytics-near-real-time","title":"Analytics Near Real Time","text":""},{"location":"projects/#technologies-used-python-kafka-streamlit","title":"Technologies Used: <code>Python, Kafka, Streamlit</code>","text":"<ul> <li>Fake data generator with Python</li> <li>Near real time processing with Kafka</li> <li>Data analysis with Streamlit</li> </ul> <p> Video </p>"},{"location":"projects/#incremental-update-with-python","title":"Incremental Update with Python","text":""},{"location":"projects/#technologies-used-python","title":"Technologies Used: <code>Python</code>","text":"<ul> <li>Creating a fake table with 1 million rows in SQL Server</li> <li>Building the Python notebook</li> <li>Full load to the destination (Postgres)</li> <li>Table comparison (Source vs. Destination)</li> <li>Change analysis</li> <li>Applying Upsert (Insert + Update)</li> <li>Validating inserted and updated data</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#python-extract-data-from-web","title":"Python Extract Data from WEB","text":""},{"location":"projects/#technologies-used-python-selenium","title":"Technologies Used: <code>Python, Selenium</code>","text":"<ul> <li>Automate file extraction</li> </ul> <p> Video</p>"},{"location":"projects/#data-quality-soda-with-alert-for-slack","title":"Data Quality Soda with Alert for Slack","text":""},{"location":"projects/#technologies-used-sql-server-big-query-soda","title":"Technologies Used: <code>SQL Server, Big Query, Soda</code>","text":"<ul> <li>Data ingestion into BigQuery</li> <li>Creating ingestion quality control with Soda</li> <li>Integration with Slack</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#testing-with-pytest","title":"Testing with Pytest","text":""},{"location":"projects/#technologies-used-python-pytest","title":"Technologies Used: <code>Python, Pytest</code>","text":"<ul> <li>Tool architecture</li> <li>Data catalog for Kafka topics</li> <li>Data catalog for Postgres tables</li> <li>Orchestration with Airflow</li> <li>Unit testing</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-catalog","title":"Data Catalog","text":""},{"location":"projects/#technologies-used-open-metadata-postgres-kafka","title":"Technologies Used: <code>Open Metadata, Postgres, Kafka</code>","text":"<ul> <li>Tool architecture</li> <li>Data catalog for Kafka topics</li> <li>Data catalog for Postgres tables</li> <li>Orchestration with Airflow</li> <li>Unit testing</li> </ul> <p> Video </p>"},{"location":"projects/#data-catalog-metadata-version","title":"Data Catalog - Metadata Version","text":""},{"location":"projects/#technologies-used-open-metadata-postgres-kafka_1","title":"Technologies Used: <code>Open Metadata, Postgres, Kafka</code>","text":"<ul> <li>Catalog with Open Metadata</li> <li>Execution of a DAG in Airflow</li> <li>Visibility into table versions</li> <li>Visibility of deleted tables</li> </ul> <p> Video </p>"},{"location":"projects/#data-ingestion-sqlserver-to-bigquery-with-python","title":"Data Ingestion SQLServer to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-sqlserver-bigquery","title":"Technologies Used: <code>Python, SQLServer, BigQuery</code>","text":"<ul> <li>Extract data from SQLServer</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-ingestion-postgres-to-bigquery-with-python","title":"Data Ingestion Postgres to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-postgres-bigquery","title":"Technologies Used: <code>Python, Postgres, BigQuery</code>","text":"<ul> <li>Extract data from Postgres</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-ingestion-mongodb-to-bigquery-with-python","title":"Data Ingestion MongoDB to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-mongodb-bigquery","title":"Technologies Used: <code>Python, MongoDB, BigQuery</code>","text":"<ul> <li>Extract data from MongoDB</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-ingestion-excel-to-bigquery-with-python","title":"Data Ingestion Excel to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-excel-bigquery","title":"Technologies Used: <code>Python, Excel, BigQuery</code>","text":"<ul> <li>Extract data from Excel</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-ingestion-csv-to-bigquery-with-python","title":"Data Ingestion CSV to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-csv-bigquery","title":"Technologies Used: <code>Python, CSV, BigQuery</code>","text":"<ul> <li>Extract data from CSV</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#data-ingestion-api-to-bigquery-with-python","title":"Data Ingestion API to BigQuery with Python","text":""},{"location":"projects/#technologies-used-python-api-bigquery","title":"Technologies Used: <code>Python, API, BigQuery</code>","text":"<ul> <li>Extract data from API</li> <li>Load data to BigQuery</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#free-sql-course","title":"Free SQL course","text":""},{"location":"projects/#technologies-used-t-sql-sql-server","title":"Technologies Used: <code>T-SQL, SQL Server</code>","text":"<ul> <li>Basic SQL course</li> <li>Installing and using SQL Server</li> <li>Best pratices with T-SQL</li> </ul> <p> Video |  Repo</p>"},{"location":"projects/#federated-query-with-trino","title":"Federated query with Trino","text":""},{"location":"projects/#technologies-used-trino-postgres-minio-sqlserver","title":"Technologies Used: <code>Trino, Postgres, Minio, SQLServer</code>","text":"<ul> <li>Creating tables in Postgres, SQL Server, and S3</li> <li>Inserting data into the three sources</li> <li>Performing a federated query with Trino</li> </ul> <p> Video |  Repo</p>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#main-skills","title":"Main skills","text":"N\u00edvel de Habilidade em Ferramentas Rating Python: 5 SQL: 5"}]}